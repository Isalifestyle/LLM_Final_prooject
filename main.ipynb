{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from databricks_langchain import ChatDatabricks, DatabricksEmbeddings\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env for Databricks credentials\n",
    "load_dotenv(override=True)\n",
    "os.environ['DATABRICKS_TOKEN'] = os.getenv('DATABRICKS_TOKEN')\n",
    "os.environ['DATABRICKS_HOST'] = os.getenv('DATABRICKS_HOST')\n",
    "\n",
    "# Initialize Databricks LLM and Embedding\n",
    "chat_model = ChatDatabricks(endpoint=\"gpt-4o-mini\", temperature=0.1, max_tokens=500)  # Reduced from 2000\n",
    "embedding_model = DatabricksEmbeddings(endpoint=\"ada-002\")\n",
    "\n",
    "# Load FAISS and metadata\n",
    "df_pd = pd.read_parquet(\"metadata.parquet\")\n",
    "index = faiss.read_index(\"my_faiss.index\")\n",
    "\n",
    "@dataclass\n",
    "class MyState:\n",
    "    chat_history: List[str] = field(default_factory=list)\n",
    "    query: Optional[str] = None\n",
    "    core_question: Optional[str] = None\n",
    "    retrieved_docs: List[str] = field(default_factory=list)\n",
    "    final_response: Optional[str] = None\n",
    "    keywords: List[str] = field(default_factory=list)  # <-- ADD THIS\n",
    "    relevance_score: Optional[float] = None  # Optional if you're tracking relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_query_node(state: MyState):\n",
    "    # Limit chat history to last 2 turns\n",
    "    state.chat_history = state.chat_history[-2:]\n",
    "\n",
    "    hcot_query = \"\"\"\n",
    "    You are an expert in understanding and rephrasing user service queries based on chat history.\n",
    "    Your goal is to break down the user's intent and context into a concise, reformulated question.\n",
    "\n",
    "    Based on the chat history: {chat_history}\n",
    "    Current query: {input}\n",
    "\n",
    "    Reformulated question:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=hcot_query, input_variables=[\"chat_history\", \"input\"])\n",
    "    core_question_chain = prompt | chat_model\n",
    "    core_question = core_question_chain.invoke({\n",
    "        \"chat_history\": \"\\n\".join(state.chat_history),\n",
    "        \"input\": state.query\n",
    "    }).content\n",
    "    state.core_question = core_question\n",
    "    return state\n",
    "\n",
    "\n",
    "### Let's **improve** the parser to handle this more **robustly**, even with weird formatting:\n",
    "\n",
    "\n",
    "### **Updated `extract_response_from_string` Function**:\n",
    "\n",
    "def extract_response_from_string(json_string):\n",
    "    json_content = json_string.strip().strip('```').replace('json', '').strip()\n",
    "    try:\n",
    "        json_pattern = re.compile(r'{.*}', re.DOTALL)\n",
    "        match = json_pattern.search(json_content)\n",
    "        if match:\n",
    "            json_obj = json.loads(match.group())\n",
    "            return json_obj.get(\"response\", \"No valid response found.\")\n",
    "        else:\n",
    "            print(\"No JSON block found in text.\")\n",
    "            return \"No JSON found in response.\"\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error: {e}\")\n",
    "        return \"Failed to parse JSON.\"\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_chunks_node(state: MyState):\n",
    "    query_vector = embedding_model.embed_query(state.core_question)\n",
    "    query_vector = np.array(query_vector).astype(\"float32\").reshape(1, -1)\n",
    "    distance, indices = index.search(query_vector, k=2)  # Limit to 2 docs\n",
    "    retrieved_chunks = df_pd.iloc[indices[0]]['file_content'].tolist()\n",
    "    # Truncate each retrieved chunk to max 300 characters\n",
    "    truncated_chunks = [chunk[:300] for chunk in retrieved_chunks]\n",
    "    state.retrieved_docs = truncated_chunks\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_response_node(state: MyState):\n",
    "    # Limit chat history length\n",
    "    state.chat_history = state.chat_history[-2:]\n",
    "\n",
    "    # Limit total combined retrieved docs length\n",
    "    max_combined_length = 1200  # Characters\n",
    "    combined_docs = \" \".join(state.retrieved_docs)\n",
    "    if len(combined_docs) > max_combined_length:\n",
    "        combined_docs = combined_docs[:max_combined_length] + \"...\"\n",
    "\n",
    "    user_query = state.core_question + \"\\n\" + combined_docs\n",
    "\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Prompt length: {len(user_query)} characters\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    prompt_prefix = \"\"\"\n",
    "Respond strictly in JSON format.\n",
    "\n",
    "Use this structure:\n",
    "{{\n",
    "  \"response\": \"answer with inline citations after each fact\",\n",
    "  \"confidence\": \"a number from 1 to 10\",\n",
    "  \"reason\": \"brief reason for your confidence\"\n",
    "}}\n",
    "\n",
    "Context:\n",
    "####{user_query}####\n",
    "\"\"\"\n",
    "\n",
    "    final_prompt = PromptTemplate(template=prompt_prefix, input_variables=[\"user_query\"])\n",
    "    final_response_chain = final_prompt | chat_model\n",
    "    final_response = final_response_chain.invoke({\"user_query\": user_query})\n",
    "\n",
    "    # Handle None case\n",
    "    if final_response is None or final_response.content is None:\n",
    "        print(\"⚠️ LLM returned None.\")\n",
    "        state.final_response = '{\"response\": \"LLM did not return any content.\", \"confidence\": 0, \"reason\": \"No output.\"}'\n",
    "    else:\n",
    "        state.final_response = final_response.content\n",
    "\n",
    "    # Print raw output\n",
    "    print(\"=\" * 30)\n",
    "    print(\"RAW LLM Response:\")\n",
    "    print(state.final_response)\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    # Update chat history\n",
    "    response_text = extract_response_from_string(state.final_response)\n",
    "    state.chat_history.append(f\"User: {state.query}\")\n",
    "    state.chat_history.append(f\"Assistant: {response_text}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def extract_keywords_node(state: MyState):\n",
    "    keyword_prompt = \"\"\"\n",
    "    Extract important keywords from this query for retrieval: {query}\n",
    "    Only list the keywords separated by commas.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=keyword_prompt, input_variables=[\"query\"])\n",
    "    keyword_chain = prompt | chat_model\n",
    "    keywords = keyword_chain.invoke({\"query\": state.query}).content\n",
    "    state.keywords = [k.strip() for k in keywords.split(\",\")]\n",
    "    return state\n",
    "\n",
    "\n",
    "def keyword_search_node(state: MyState):\n",
    "    keyword_docs = []\n",
    "    for keyword in state.keywords:\n",
    "        matches = df_pd[df_pd['file_content'].str.contains(keyword, case=False, na=False)]\n",
    "        keyword_docs.extend(matches['file_content'].tolist())\n",
    "\n",
    "    # Limit keyword results and truncate\n",
    "    keyword_docs = [doc[:300] for doc in keyword_docs[:2]]  # Top 2 keyword matches, each 300 chars\n",
    "\n",
    "    # Combine with FAISS retrieved docs\n",
    "    combined_docs = list(set(state.retrieved_docs + keyword_docs))[:4]  # Max 4 combined docs\n",
    "    state.retrieved_docs = combined_docs\n",
    "    return state\n",
    "\n",
    "def evaluate_relevance_node(state: MyState):\n",
    "    query_vector = embedding_model.embed_query(state.core_question)\n",
    "    doc_vectors = [embedding_model.embed_query(doc) for doc in state.retrieved_docs]\n",
    "    similarities = [np.dot(query_vector, doc_vec) / (np.linalg.norm(query_vector) * np.linalg.norm(doc_vec)) for doc_vec in doc_vectors]\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    state.relevance_score = avg_similarity  # Optional for reporting\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph: Define Graph and Flow\n",
    "workflow = StateGraph(MyState)\n",
    "\n",
    "workflow.add_node(\"extract_keywords\", RunnableLambda(extract_keywords_node))\n",
    "workflow.add_node(\"reformulate_query\", RunnableLambda(reformulate_query_node))\n",
    "workflow.add_node(\"retrieve_chunks\", RunnableLambda(retrieve_chunks_node))\n",
    "workflow.add_node(\"keyword_search\", RunnableLambda(keyword_search_node))\n",
    "workflow.add_node(\"generate_response\", RunnableLambda(generate_response_node))\n",
    "workflow.add_node(\"evaluate_relevance\", RunnableLambda(evaluate_relevance_node))\n",
    "\n",
    "workflow.set_entry_point(\"extract_keywords\")\n",
    "workflow.add_edge(\"extract_keywords\", \"reformulate_query\")\n",
    "workflow.add_edge(\"reformulate_query\", \"retrieve_chunks\")\n",
    "workflow.add_edge(\"retrieve_chunks\", \"keyword_search\")\n",
    "workflow.add_edge(\"keyword_search\", \"generate_response\")\n",
    "workflow.add_edge(\"generate_response\", \"evaluate_relevance\")\n",
    "workflow.add_edge(\"evaluate_relevance\", END)\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Prompt length: 1338 characters\n",
      "==============================\n",
      "==============================\n",
      "RAW LLM Response:\n",
      "```json\n",
      "{\n",
      "  \"response\": \"Yes, it is possible to change the 'ExpSQLSvc' and 'ExpSQLAgtSvc' accounts from local to domain users without performing a complete rebuild. This can typically be done through the service properties in the Windows Services management console, where you can specify the account under which the service runs. However, it is important to ensure that the domain accounts have the necessary permissions and rights to run the services effectively (Honeywell, 2010). Additionally, proper configuration and testing should be conducted to avoid service disruptions (Honeywell, 2019).\",\n",
      "  \"confidence\": 8,\n",
      "  \"reason\": \"The information is based on standard practices for Windows services and the context provided suggests familiarity with service management.\"\n",
      "}\n",
      "```\n",
      "==============================\n",
      "Yes, it is possible to change the 'ExpSQLSvc' and 'ExpSQLAgtSvc' accounts from local to domain users without performing a complete rebuild. This can typically be done through the service properties in the Windows Services management console, where you can specify the account under which the service runs. However, it is important to ensure that the domain accounts have the necessary permissions and rights to run the services effectively (Honeywell, 2010). Additionally, proper configuration and testing should be conducted to avoid service disruptions (Honeywell, 2019).\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# Set initial user query\n",
    "initial_state = MyState(\n",
    "    chat_history=[],\n",
    "    query=\"Do you know of a method, which does not involve a total rebuild, for changing the ‘ExpSQLSvc’ and ‘ExpSQLAgtSvc’ accounts from local to domain users, please?\"\n",
    ")\n",
    "# Run the graph\n",
    "final_state = graph.invoke(initial_state)\n",
    "\n",
    "# Print the final response\n",
    "print(extract_response_from_string(final_state[\"final_response\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
